#
# Example slurm.conf file generated by DeepOps. 
#
# Slurm provides configurator.html
# (in doc/html) to build a configuration file customized
# for your environment.
#
# See the slurm.conf man page for more information.

# Define a name for the cluster
ClusterName={{ slurm_cluster_name }}

# Configure the controllers
{% if slurm_enable_ha %}
{% for node_name in groups['slurm-master'] %}
SlurmctldHost={{ node_name }}
{% endfor %}
StateSaveLocation={{ slurm_ha_state_save_location }}
{% else %}
SlurmctldHost={{ groups['slurm-master'][0] }}
StateSaveLocation=/var/spool/slurm/ctld
{% endif %}

# Basic configuration
SlurmUser={{ slurm_username }}
SlurmctldPort=6817
SlurmdPort=6818
AuthType=auth/munge
#JobCredentialPrivateKey=
#JobCredentialPublicCertificate=
SlurmdSpoolDir=/var/spool/slurm/d
SwitchType=switch/none
SlurmctldPidFile=/var/run/slurmctld.pid
SlurmdPidFile=/var/run/slurmd.pid
ProctrackType=proctrack/cgroup
PluginDir={{ slurm_install_prefix }}/lib/slurm
#FirstJobId=
#MaxJobCount=
#PlugStackConfig=
#PropagatePrioProcess=
#PropagateResourceLimits=
#PropagateResourceLimitsExcept=
PropagateResourceLimitsExcept=MEMLOCK

# Basic job behavior
ReturnToService={{ slurm_return_to_service }}
RebootProgram="{{ slurm_reboot_program }}"
ResumeTimeout={{ slurm_resume_timeout }}
JobFileAppend=1

# Fallback memory allocation values
DefMemPerCPU=3800
DefMemPerGPU=40000
DefCpuPerGpu=10

{% if slurm_include_pmix %}
# Use PMIX as our default MPI configuration
MpiDefault=pmix
{% endif %}

# Prolog/epilog config
{% if slurm_contain_ssh is defined %}
PrologFlags=Alloc,Serial,Contain
{% else %}
PrologFlags=Alloc,Serial
{% endif %}
{% if slurm_enable_prolog_epilog %}
Prolog={{ slurm_config_dir }}/prolog.sh
Epilog={{ slurm_config_dir }}/epilog.sh
{% endif %}
#SrunProlog=
#SrunEpilog=
#TaskProlog=
#TaskEpilog=

{% if slurm_health_check_program is defined %}
# Health checking
HealthCheckProgram={{ slurm_health_check_program }}
HealthCheckInterval=300
HealthCheckNodeState=IDLE
{% endif %}

# Mail program to use
MailProg=/usr/bin/s-nail

TaskPlugin=affinity,cgroup
#TrackWCKey=no
#TreeWidth=50
#TmpFS=
#UsePAM=

# TIMERS
SlurmctldTimeout=120
SlurmdTimeout=300
InactiveLimit=0
MinJobAge=300
KillWait=30
Waittime=0

# SCHEDULING
SchedulerType=sched/backfill
#SchedulerAuth=
SelectType=select/cons_tres
SelectTypeParameters=CR_Core_Memory,CR_CORE_DEFAULT_DIST_BLOCK,CR_ONE_TASK_PER_CORE
#PriorityType=priority/multifactor
#PriorityDecayHalfLife=14-0
#PriorityUsageResetPeriod=14-0
#PriorityWeightFairshare=100000
#PriorityWeightAge=1000
#PriorityWeightPartition=10000
#PriorityWeightJobSize=1000
#PriorityMaxAge=1-0

PreemptMode=REQUEUE
PreemptType=preempt/partition_prio

# LOGGING
SlurmctldDebug=3
SlurmctldLogFile=/var/log/slurm/slurmctld.log
SlurmdDebug=3
SlurmdLogFile=/var/log/slurm/slurmd.log
JobCompType=jobcomp/none
#JobCompLoc=

# ACCOUNTING
JobAcctGatherType=jobacct_gather/cgroup
#JobAcctGatherFrequency=30
{% if slurm_manage_gpus %}
AccountingStorageTRES=gres/gpu
{% endif %}
#DebugFlags=CPU_Bind,gres
AccountingStorageType=accounting_storage/slurmdbd
AccountingStorageHost={{ groups["slurm-master"][0] }}
#AccountingStorageLoc=
#AccountingStorageEnforce=associations,limits,qos
AccountingStorageUser={{ slurm_db_username }}
AccountingStoragePass=/var/run/munge/munge.socket.2

# COMPUTE NODES
{% if slurm_manage_gpus %}
GresTypes=gpu
{% endif %}
{% for node_name in groups['slurm-node'] %}
{% set memory =  hostvars[node_name]["ansible_local"]["memory"] -%}
{% set cpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["cpu_topology"] -%}
{% set gpu_topology =  hostvars[node_name]["ansible_local"]["topology"]["gpu_topology"] -%}
    NodeName={{ node_name }}{{ " " -}}
    {% if slurm_manage_gpus %} {%- if gpu_topology|count %} Gres=gpu:{{ gpu_topology|count }} {% endif -%} {% endif %}
    CPUs={{ cpu_topology.logical_cpus|int }}{{ " " -}}
    Sockets={{ cpu_topology.sockets|int }}{{ " " -}}
    CoresPerSocket={{ cpu_topology.cores_per_socket|int }}{{ " " -}}
    ThreadsPerCore={{ cpu_topology.threads_per_core }}{{ " " -}}
    Procs={{ cpu_topology.sockets|int * cpu_topology.cores_per_socket|int }}{{ " " -}}
    RealMemory={{ memory.total_mb|int }}{{ " " -}}
    State=UNKNOWN
{% endfor %}

include /etc/slurm/slurm.conf.d/{{ inventory_file | basename }}.conf
